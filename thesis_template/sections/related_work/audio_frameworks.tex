%!TEX root = ../../thesis.tex

\section{HARK}
\label{related:frameworks}
In this part \gls{hark} \cite{Nakadai_2017jrm} will be discussed, a modular framework with explicit focus on integrating beamforming and speech recognition solutions, developed by a team of researcher centered around Kyoto University.
Its goal is to provide an all in one solution for audition in robots specifically.
\gls{hark} is modular as it provides several algorithms (so called nodes) for each processing step.
These are centered around \gls{ssl}, beamforming, filtering, and feature extraction nodes.
\gls{hark} does not directly incorporate \gls{asr} solutions, but instead opts to extract audio features and send them via network to dedicated programs.
These are thusly not under its direct control.
It does not directly support other kinds of audio analysis, such as emotion-, voice-, or gender-recognition.
Nevertheless, one could develop these kinds of nodes to work with audio received via network and thus ''trick'' \gls{hark} to incorporate these nodes.
Naturally, it thus does not support the synchronization of results of any such components.
All the more, it does not support the synchronization of \gls{ssl} and \gls{asr} results, as results of \gls{asr} nodes lie outside its control.

\gls{hark} will take care of transporting audio data in between processing steps, but mostly transfers them in a frequency based representation, not as raw audio data.
This results in a quite stripped down nodes, as it reduces the number of fast Fourier transforms needed to flat one instead of one per node relying on them.
Nodes which rely on raw audio however, must either manually restore the signal or use a specially developed node for this task.
Both variants results in a degrading signal however.

%-----------------------------------------------------------------------
