%!TEX root = ../../thesis.tex

\section{HARK}
\label{related:frameworks}
In this part we will discuss \gls{hark} \cite{Nakadai_2017jrm}, a modular framework with explicit focus on integrating beamforming and speech recognition solutions, developed by a team of researcher centered around Kyoto University.
Its goal is to provide an all in one solution for audition in robots specifically.
\gls{hark} is modular as it provides several algorithms (so called nodes) for each processing step.
These are centered around \gls{ssl}, beamforming, filtering, and feature extraction nodes.
\gls{hark} does not directly incorporate \gls{asr} solutions, but instead opts to extract audio features and send them via network to dedicated programs.
These are thusly not under its direct control.
It does not directly support other kinds of audio analysis, such as emotion-, voice-, or gender-recognition.
Nevertheless, one could develop these kinds of nodes to work with audio received via network and thus ''trick'' \gls{hark} to incorporate these nodes.
Naturally, it thus does not support the synchronization of results of any such components.
All the more, it does not support the synchronization of \gls{ssl} and \gls{asr} results, as results of \gls{asr} nodes lie outside its control.

\gls{hark} will take care of transporting audio data in between processing steps, but mostly transfers them in a frequency based representation, not as raw audio data.
This results in a quite stripped down nodes, as it reduces the number of fast Fourier transforms needed to flat one instead of one per node relying on them.
Nodes which rely on raw audio however, must either manually restore the signal or use a specially developed node for this task.
Both variants results in a degrading signal however.

%-----------------------------------------------------------------------


%\section{Audio frameworks}
%In this part we will discuss several frameworks used for audio processing.
%HARK will be discussed in greater detail.
%The four other frameworks we will discuss here, namely JACK, ALSA, Pulseaudio and Gstreamer are similar enough to JACK to justify just discussing their differences to JACK.
%
%\subsubsection{JACK}
%- The Jack audio connection kit \cite{JACK} is a protocol for audio transmission between components with the explicit requirement for a low latency, to enable real time capture, playback and editing of sound. 
%
%- JACK does not allow for additional information to transmitted with the raw audio it transports, so any additionally information must be transmitted and synchronized separately with another middleware. 
%
%- JACK is able to transmit audio over network, but generally requires all participating computers to run an instance of the same server implementation. 
%
%- JACK requires all components which use its API to process audio in a specific and rather short time. 
%Audio is written to and read from JACK via buffers, which will be in turn read and written by JACK in specific intervals.
%If a component is not able to read or produce audio fast enough, JACK will overwrite or read old audio respectively, so audio is lost. 
%This is important for applications which result in humans hearing the transmitted audio directly, to reduce latency and negate long or at least noticeable periods of silence.
%
%\subsection{Gstreamer}
%
%Gstreamer \cite{Gstreamer}
%
%\subsection{ALSA}
%The Advanced Linux Sound Architecture (ALSA) 
%It provides drivers and access to hardware sound sources and sinks, typically -but not limited to- microphones and speakers.
%
%\begin{itemize}
%	\item de facto standard on every linux machine, but available on several other operating systems
%	\item provides access to hardware sound sources and sinks
%\end{itemize}
%
%\subsection{PulseAudio}
%
%\subsection{Closing remarks}
%Most of the presented frameworks can be used alongside each other, as they oftentimes have interfaces for each other. 
%A combination of the presented frameworks may come to mind but most of the discussed problems will just add up instead of cancel each other out.
