%!TEX root = ../../thesis.tex
%=============================================================================

\section{Robocup Speech Recognition Test}
\label{eval:task_start}
In this section, an evaluation of the proposed pipeline in a real world scenario will be presented.
The goal of this evaluation is to assess if using the proposed pipeline leads to improved recognition results, in this particular case to increase the accuracy of sound source localization results.
These could be achieved through the result synchronization the proposed pipeline provides.

This evaluation will be based on the Speech \& Person Recognition task of RoboCup@home 2018 \cite{rulebook_2018}, although some minor modifications will be made to the setup.
The evaluation will be conducted on the Pepper robot (TODO cite).

\subsubsection{Software Setup}
Two distinct solutions will be benchmarked:
The existing RoboCup setup and a reimplementation of it within the proposed pipeline.
Both will mainly be comprised of a sound source localization, a speech recognizer and a layer to combine the results of both these components.

\begin{figure}[]
	\centering
	\includegraphics[width=0.66\textwidth]{diagrams/eval_task_proposed.pdf}
	\caption{Setup of of proposed pipeline.
		Audio is only acquired from \gls{alsa} at one point, then fed into \gls{ssl} and speech recognizer separately.
		Generated Information is gathered by the Orchestrator and provided after aggregation to a logging component.}
	\label{pic:eval_task_setup_new}
\end{figure}

The proposed pipeline (see figure \ref{pic:eval_task_setup_new}) will consist mainly of the basic speech recognition setup established as the baseline for the data set evaluation (see chapter \ref{eval:dataset:pipeline:baseline}), i.e. an audio grabber to record audio from a microphone as well as the previously (see chapter \ref{eval:dataset:setup}) established \gls{vad}, \gls{ps} speech recognizer and Orchestrator.
As sound source localization the component using \texttt{pyroomacoustics} described earlier in chapter \ref{main:components:ssl}, will be employed.
Due to the robot providing four channel audio from its four microphones, which are all needed by the \gls{ssl}, but the speech recognition part of the pipeline requiring only one channel audio, an additional component is needed in the form of a channel splitter.
Since one of the features of the proposed pipeline is synchronization of results, recorded speech recognition results will naturally be enriched with corresponding \gls{ssl} results.

\begin{figure}[]
	\centering
	\includegraphics[width=0.5\textwidth]{diagrams/eval_task_old.pdf}
	\caption{Setup of existing solution. 
		Audio is acquired from \gls{alsa} by both the speech recognizer and the \gls{ssl} component independently. 
		Information from both is then collected and combined by Bonsai.
		Bonsai's output is then logged.}
	\label{pic:eval_task_setup_old}
\end{figure}

For the existing solution (see figure \ref{pic:eval_task_setup_old}), the already established \gls{psa} will take the role of the speech recognizer.
For sound source localization, a slightly modified variant of the proposed pipelines component will be employed, as elaborated earlier (see chapter \ref{main:components:ssl}).
To combine both results, a behavior controller called Bonsai \cite{bonsai} will be used.
Bonsai is a TODO.
An channel splitter similar to the proposed pipeline is not necessary, as \gls{alsa} will assume this task for the \gls{psa}.
Since result synchronization is not a feature of primary concern for Bonsai, its default behavior regarding this will be employed, which is to aggregate the \gls{ssl} results from two seconds to half a second before a speech recognition result was received.

Naturally, similar components within both solutions will be configured equivalently.
The output of both solutions will be logged and contain \gls{ssl} angles and recognized speech.
Both solutions will be run concurrently, to eliminate run-to-run variances between the solutions.
To reduce reciprocal influence, the existing RoboCup setup will run on the robot, while the proposed pipeline will run on a secondary machine, except for the audio grabber node, which must run on the robot.

\subsubsection{Description of the RoboCup@home Task}
The RoboCup task would normally start with a small visual person recognition part.
Since visual perception is not part of this thesis, this part of the task will be left out.
The task will instead start with the \textit{Riddles Game}, in which an operator standing in front of the robot will ask the robot five questions which it should answer.

Afterwards, between five and ten persons would surround the robot in the \textit{Blind Mans Bluff Game}.
Five random persons would then ask the robot each an additional questions.
The robot should turn to the speakers and answer them.
To ease the evaluation of \gls{ssl} results, only four persons will partake in the \textit{Blind Mans Bluff Game} of this evaluation.
One person each will stand directly in front of the robot, behind the robot and to either side of the robot, with circa a 90$^\circ$ shift in angle with respect to the initial orientation of the robot (see figure \ref{pic:eval_task}).
All questions for the task will be generated from the publicly available RoboCup 2018 command generator (TODO cite).

\begin{figure}[]
	\centering
	\includegraphics[width=0.66\textwidth]{diagrams/robocup_task_t1.pdf}
	\caption{Test scenario for the RoboCup task, specifically the \textit{Blind Mans Bluff Game}. 
		One person each is locate in front, behind and to either side of the robot.
		Here, the person on the right is asking the robot a question.}
	\label{pic:eval_task}
\end{figure}

Simultaneous execution of both solutions requires small changes to the task.
Since the robot can not turn to two different angles at once, instead of turning the robot to the speaker, logged angle information was used to evaluate \gls{ssl} results.
For similar reasons and since producing answers to the questions can be done rather easily, recognizing the correct question was counted as correctly answered.
Both these information can easily be acquired by inspecting the logged output of both solutions.

To ease scoring of the task and provide documentation, the task was recorded by a video camera.
The task was conducted eight times, but two of these runs were excluded from the evaluation, due to camera malfunctions.
The recordings of the included runs were made available\footnote{\url{https://youtu.be/TrodNlUkQPM}}.

To simulate the environment preset on most RoboCup events, which are typically conducted in a crowded convention center and to provide non optimal conditions for the sound source localization, extensive noise was generated in front of the robot by two speakers.
This noise is intended to significantly increase the difficulty of \gls{ssl}, to provide more meaningful results.
A still image of the video recordings can be seen in figure \ref{pic:eval_task_setup_pepper}.
The two speakers seen in the foreground were playing restaurant noises, which consisted mostly of human chatter.

\begin{figure}[]
	\centering
	\includegraphics[width=\textwidth]{bilder/eval/pepper_task_setup_small.jpg}
	\caption{Task evaluation setup. 
		Speakers can be seen in the foreground, while the test crowd shuffles around in between runs.}
	\label{pic:eval_task_setup_pepper}
\end{figure}

Pinpoint accuracy of \gls{ssl} results is generally not required.
For the purposes of locating and differentiating speakers from one another, especially in a household environment, where generally only a handful of people are present, any \gls{ssl} result within a margin of error of 15$^\circ$ can be regarded as correct.
Rather erroneous detections caused by noise before or after the spoken question should not falsely be included in an analysis on where a speaking person is located.
As such, for the purpose of scoring both pipelines' results, any \gls{ssl} result would be regarded as correct, if it was below the proposed 15$^\circ$ margin of error.
Results were manually evaluated by combining information from the recorded videos, spoken questions and logged \gls{ssl} recordings.

\subsubsection{Discussion}
The performance of the existing solution and our pipeline can be seen in figure \ref{table:eval_task_results_old}.
When comparing results of both solutions, it can easily be seen that similarly to the data set evaluation (see chapter \ref{eval:discussion}), speech recognition results of the proposed pipeline outperform those of the existing solution.
%This could be expected, considering that both pipelines use the same speech recognizer as in the data set evaluation.

\begin{figure}[]
	\begin{tabular}{ | l !{\vrule width 2pt} r | l !{\vrule width 1pt} r | l !{\vrule width 1.5pt} r | l !{\vrule width 1.5pt} r | l |}
		\hline
		Run & 
		\multicolumn{2}{ p{3.5cm} !{\vrule width 1pt}}{Correctly recognized sentences} &
		\multicolumn{2}{c !{\vrule width 1.5pt}}{Points} &
		\multicolumn{2}{c !{\vrule width 1.5pt}}{Points by \gls{ssl}} &
		\multicolumn{2}{c |}{Avg \gls{ssl} Error} \\ \cline{2-9}  
		& Old  & New	& Old&New &	Old&New &	Old & New \\\hline
		1 & 50\% & 65\% & 45 & 50 & 0  &  0 & 83.4 & 86.5\\ \hline
		2 & 50\% & 55\% & 40 & 70 & 10 & 20 & 92.1 & 46.6\\ \hline
		4 & 35\% & 55\% & 25 & 35 &  0 &  0 & 116.0 & 87.9\\ \hline
		5 & 40\% & 55\% & 30 & 50 & 10 & 20 & 98.4 & 54.3\\ \hline
		6 & 45\% & 60\% & 40 & 60 & 10 &  0 & 97.6 & 82.3\\ \hline
		7 & 60\% & 60\% & 50 & 60 & 10 & 10 & 105.2 & 51.9\\ \hline \hline
		Avg & 46.7\% & 58.3\% & 38.3 & 54.2 & 6.7 & 8.3 & 98.78 & 68.25 \\
		\hline
	\end{tabular}
	\caption{Results of the pre-existing (Old) and our pipeline (New) in the RoboCup task by run.
		Run 3 and 8 were omitted because of camera malfunctions.}
	\label{table:eval_task_results_old}
\end{figure}

The \gls{ssl} results of both solutions may appear quite weak, but one should keep the noise in mind which was specifically generated to increase the difficulty of \gls{ssl}.
In a real world scenario, similar noise may come from a loud dishwasher or TV.
A robot should however still be able to locate a person giving it commands when confronted by such challenging environments.

The proposed pipeline provided slightly more correct \gls{ssl} results, which is to say it produced five opposed to the four of the existing solution where 30 were possible.
Both factor in when comparing the actual scores of the RoboCup task, where the proposed pipeline outperforms the existing solution.
However, I consider this score to not be very meaningful, as most of it is caused by the proposed pipeline's speech recognizer performing better.

Additionally, information about the average error of the \gls{ssl} results were generated, as I consider these in this particular case to be more significant to the evaluation due to the score results being very close to one another.
When comparing them, the proposed pipeline outperforms the existing solution in all runs but one, oftentimes by a significant margin.
In run seven and two it (nearly) halves the error.
Overall, the proposed pipeline reduces the error in angle by around 30$^\circ$ in comparison to the existing solution.
We believe this to be the case due to the proposed pipelines enhanced synchronization abilities.
Within the existing solution, shorter questions' \gls{ssl} results may be random as Bonsai's \gls{ssl} lookup time frame can completely miss the actual time in which the question was asked.

To sum up, a number of key insights could be gained by this experiment:

\begin{enumerate}
	\item Our pipeline provides \gls{ssl} results which better match the observed real world 
\end{enumerate}














