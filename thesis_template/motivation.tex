%!TEX root = thesis.tex

\chapter{Introduction \& Motivation}
\label{motiv:start}
Robotics in general depend heavily on \gls{hri}, which in turn is influenced and shaped by human-human interaction. %todo cite
Since speech is one of the most important form of communication between humans, it is no surprise that it is also one of the most important parts of \gls{hri}.
From a robots point of view, speech can be divided into speech generation and the main interest of this thesis: speech understanding.

Being able to perfectly understand the words a person spoke does not completely cover speech recognition.
See for example this video\footnote{\url{https://www.youtube.com/watch?v=iEMKZdwJPE8}} provided or stills from it in figure \ref{pic:moti:imustgonow}.
In the video parts of a RoboCup@home Task of the RoboCup World Championship 2018 in Montreal can be seen.
In three different instances two referees are giving a robot commands, but they take turns in speaking to the robot.
The robot does not acknowledge this nor does it seem to notice.
More so: When asking for the confirmation of a specific command, it does not even seem to care that a different person gives this confirmation, or that the original referee walked away and no longer partakes in the conversation.

\begin{figure}[]
	\centering
	\includegraphics[width=.8\textwidth]{bilder/motivation/intro_1_edit.png}\\\vspace{3pt}
	\includegraphics[width=.8\textwidth]{bilder/motivation/intro_2_edit.png}\\\vspace{3pt}
	\includegraphics[width=.8\textwidth]{bilder/motivation/intro_3_edit.png}\\\vspace{3pt}
	\includegraphics[width=.8\textwidth]{bilder/motivation/intro_4_edit.png}\\\vspace{3pt}
	
	\caption{Interaction between two referees and a robot in the RoboCup@home league.
		The referee in blue abandons the robot mid interaction, which does not acknowledge this at all.}
	\label{pic:moti:imustgonow}
\end{figure}

Potential security risks aside, the shown interactions appear unnatural:
The robot does not seem to really perceive the human, instead it is quite clear that it just listens for a particular combination of words.
A similar phenomenon could very publicly be observed in the near past, when Amazons Alexa ordered a variety of objects online, after hearing commands from a  TV\footnote{\url{http://archive.is/zXuJu}}.

In social robots, this behavior is not acceptable, especially since the means to at least partially solve these problems already exist.
Voice recognition technologies which can differentiate voices to a degree most people could be identified are available (todo cite voice github project).
Additionally, computer vision can be used to search for speakers, either standalone, looking for moving lips (too cite), or in combination with sound source localization (see \ref{basics:ssl}).
We will later show (see chapter \ref{related:robocup}) these to not be in use by leading robotics teams.

Of course suitable robot behaviors need to be created, to take all this information into account.
When creating these behaviors, one must consider how these information are fed into them.
The behavior in question must either be fed a pre-combined package or be able to combine the information, e.g. a spoken utterance and a distinct, detected voice.
If the behavior handles the combination, both may not be fed into the behavior at the same time, resulting in the problem to combine them.
A number of factors have to be considered when combining these information.
Information may arrive in ratios which are not be distributed evenly.
While just a single, long speech utterance may be received, a number of voices can be detected, or several results of the same voice.
Results thus have to be combined in a manner that takes this into account.
Additionally, results will have different calculation times, based on what component created them and thus may be temporally unaligned.
This must be considered as well, creating the need to synchronize results based on their occurrence in time.

The problems just presented are clearly out of scope for robot behaviors an thus should be handled separately.
We thus declare the main goal of this thesis to create a framework for the automatic generation of these synchronized audio analysis results.

Furthermore, a number of secondary goals can be declared.
Naturally any approach to synchronize audio results shall not have a negative impact on these results.
This can be specified in two more concrete terms.
First and foremost, the accuracy of the results shall not decrease by being incorporated in the proposed solution.
Second, synchronizing the results shall not take significantly longer to compute than not synchronizing them.
%todo elaborate

%---------------------------------- modularity secnd goal

Robotics is a field of intensive current research, and in the last years a number of leaps have been made.
These include, but are no limited to OpenPose (todo cite https://github.com/CMU-Perceptual-Computing-Lab/openpose), YOLO (todo cite https://pjreddie.com/darknet/yolo/) and DeepSpeech (todo cite https://github.com/mozilla/DeepSpeech).
Modularity and the ability to include such advances without the need to completely overhaul an existing system greatly decreases the time needed to incorporate such new technologies.
In turn, research speed can greatly benefit.
We can thus declare modularity a secondary goal, to increase the usability of the proposed framework.

%-----------------------------------

The remainder of this thesis is structured as follows:
We will first introduce a number of concepts and frameworks in chapter \ref{basics:start}.
After this I will present my solution in chapter \ref{main:main}.
We will then proceed to evaluate my proposed solution in chapter \ref{eval} based on two experiments.
Lastly I will summarize my findings and point out possible future work in chapter \ref{conclusion}.
