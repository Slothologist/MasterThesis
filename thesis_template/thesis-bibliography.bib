@misc{JACK,
	title={Jack Audio Connection Kit},
	author={Paul Davis AND Stéphane Letz AND others},
	year={2002},
	howpublished={Open Source Project},
	note={\url{http://jackaudio.org/}}
}

@misc{Gstreamer,
	title={Gstreamer},
	author={Erik Walthinsen AND others},
	month={1},
	year={2001},
	howpublished={Open Source Project},
	note={\url{https://gstreamer.freedesktop.org/}}
}

@misc{rulebook_2018,
	author = {Matamoros, Mauricio AND Rascon, Caleb AND Hart, Justin AND Holz, Dirk AND van Beek, Loy },
	title = {RoboCup@Home 2018: Rules and Regulations},
	pages = {64 -- 67},
	year = {2018},
	howpublished = {\url{http://www.robocupathome.org/rules/2018_rulebook.pdf}},
}

@misc{psutil,
	title={psutil},
	author={Giampaolo Rodola AND others},
	year={2013},
	howpublished={Open Source Project},
	note={\url{https://psutil.readthedocs.io}}
}

@misc{ToBi,
	title={Team of Bielefeld},
	year={2009},
	howpublished={RoboCup@Home Participants},
	note={\url{https://www.cit-ec.de/en/tobi}}
}

@misc{GoogleSpeech,
	title={Google Cloud Speech-to-Text},
	author={Google},
	year={2013},
	month={11},
	day={6},
	howpublished={Software Product},
	note={\url{https://cloud.google.com/speech-to-text/}}
}

@misc{MicrosoftSpeech,
	title={Microsoft Speech to Text},
	author={Microsoft},
	year={2018},
	month={9},
	howpublished={Software Product},
	note={\url{https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/}}
}

@article{pyroomacoustics,
	author = {Robin Scheibler AND Eric Bezzam AND Ivan Dokmanić},
	title = {Pyroomacoustics: A Python package for audio room simulations and array processing algorithms},
	year = {2017},
	eprint = {arXiv:1710.04196},
	doi = {10.1109/ICASSP.2018.8461310},
}

@phdthesis{bonsai,
	type = {dissertation},
	title = {Behavior coordination for reusable system design in interactive robotics},
	author = {Siepmann Frederic},
	school = {Bielefeld University},
	year = {2013},
}

@article{deepspeech,
	author    = {Awni Y. Hannun and
	Carl Case and
	Jared Casper and
	Bryan Catanzaro and
	Greg Diamos and
	Erich Elsen and
	Ryan Prenger and
	Sanjeev Satheesh and
	Shubho Sengupta and
	Adam Coates and
	Andrew Y. Ng},
	title     = {Deep Speech: Scaling up end-to-end speech recognition},
	journal   = {CoRR},
	volume    = {abs/1412.5567},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.5567},
	archivePrefix = {arXiv},
	eprint    = {1412.5567},
	timestamp = {Mon, 13 Aug 2018 16:48:07 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/HannunCCCDEPSSCN14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{tensorflow,
	title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={http://tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dan~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}

@article{Nakadai_2017jrm,
	title={Development, Deployment and Applications of Robot Audition Open Source Software HARK},
	author={Kazuhiro Nakadai and Hiroshi G. Okuno and Takeshi Mizumoto},
	journal={Journal of Robotics and Mechatronics},
	volume={29},
	number={1},
	pages={16-25},
	year={2017},
	doi={10.20965/jrm.2017.p0016},
}

@INPROCEEDINGS{287895,
	author={A. T. {Alouani} and T. R. {Rice}},
	booktitle={Proceedings of 26th Southeastern Symposium on System Theory},
	title={On asynchronous data fusion},
	year={1994},
	pages={143-146},
	keywords={sensor fusion;delays;tracking;asynchronous data fusion;multisensor tracking system;sequential processing;inherent delay;multitasking radar;dissimilar sensors;Biosensors;Sensor systems;Sensor fusion;Acoustic sensors;Optical sensors;Laser radar;Time measurement;Delay;Radar measurements;Humans},
	doi={10.1109/SSST.1994.287895},
	month={March},
}

@article{TURK2014189,
	title = {Multimodal interaction: A review},
	journal = {Pattern Recognition Letters},
	volume = {36},
	pages = {189 - 195},
	year = {2014},
	issn = {0167-8655},
	doi = {https://doi.org/10.1016/j.patrec.2013.07.003},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865513002584},
	author = {Matthew Turk},
	keywords = {Multimodal interaction, Perceptual interface, Multimodal integration, Review},
	abstract = {People naturally interact with the world multimodally, through both parallel and sequential use of multiple perceptual modalities. Multimodal human–computer interaction has sought for decades to endow computers with similar capabilities, in order to provide more natural, powerful, and compelling interactive experiences. With the rapid advance in non-desktop computing generated by powerful mobile devices and affordable sensors in recent years, multimodal research that leverages speech, touch, vision, and gesture is on the rise. This paper provides a brief and personal review of some of the key aspects and issues in multimodal interaction, touching on the history, opportunities, and challenges of the area, especially in the area of multimodal integration. We review the question of early vs. late integration and find inspiration in recent evidence in biological sensory integration. Finally, we list challenges that lie ahead for research in multimodal human–computer interaction.},
}

@article{PORIA201650,
	title = {Fusing audio, visual and textual clues for sentiment analysis from multimodal content},
	journal = {Neurocomputing},
	volume = {174},
	pages = {50 - 59},
	year = {2016},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2015.01.095},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231215011297},
	author = {Soujanya Poria and Erik Cambria and Newton Howard and Guang-Bin Huang and Amir Hussain},
	keywords = {Multimodal fusion, Big social data analysis, Opinion mining, Multimodal sentiment analysis, Sentic computing},
	abstract = {A huge number of videos are posted every day on social media platforms such as Facebook and YouTube. This makes the Internet an unlimited source of information. In the coming decades, coping with such information and mining useful knowledge from it will be an increasingly difficult task. In this paper, we propose a novel methodology for multimodal sentiment analysis, which consists in harvesting sentiments from Web videos by demonstrating a model that uses audio, visual and textual modalities as sources of information. We used both feature- and decision-level fusion methods to merge affective information extracted from multiple modalities. A thorough comparison with existing works in this area is carried out throughout the paper, which demonstrates the novelty of our approach. Preliminary comparative experiments with the YouTube dataset show that the proposed multimodal system achieves an accuracy of nearly 80\%, outperforming all state-of-the-art systems by more than 20\%.},
}

@inproceedings{10.1117/12.138164,
	author = {W. Dale Blair and Theodore R. Rice and Brendan S. McDole and E. M. Sproul},
	title = {{Least-squares approach to asynchronous data fusion}},
	volume = {1697},
	booktitle = {Acquisition, Tracking, and Pointing VI},
	editor = {Michael K. Masten and Larry A. Stockum},
	organization = {International Society for Optics and Photonics},
	publisher = {SPIE},
	pages = {130 -- 141},
	year = {1992},
	doi = {10.1117/12.138164},
	URL = {https://doi.org/10.1117/12.138164}
}

@ARTICLE{4383603,
	author={L. P. {Yan} and B. S. {Liu} and D. H. {Zhou}},
	journal={IEEE Transactions on Aerospace and Electronic Systems},
	title={Asynchronous multirate multisensor information fusion algorithm},
	year={2007},
	volume={43},
	number={3},
	pages={1135-1146},
	keywords={recursive estimation;sensor fusion;state estimation;asynchronous multirate multisensor;optimal dynamic information fusion;state space models;state estimation;global measurements;linear minimum variance;State estimation;Signal processing algorithms;Sampling methods;Signal resolution;Sensor fusion;Recursive estimation;Stochastic processes;Image reconstruction;State-space methods},
	doi={10.1109/TAES.2007.4383603},
	month={July},
}

@ARTICLE{1468761,
	author={A. T. {Alouani} and J. E. {Gray} and D. H. {McCabe}},
	journal={IEEE Transactions on Aerospace and Electronic Systems},
	title={Theory of distributed estimation using multiple asynchronous sensors},
	year={2005},
	volume={41},
	number={2},
	pages={717-722},
	keywords={sensor fusion;target tracking;synchronisation;distributed estimation;multiple asynchronous sensors;track fusion;linear fusion rule;batch processing;Bar-Shalom-Campo fusion rule;Estimation theory;Sensor fusion;State estimation;Computer architecture;Delay estimation;Computational efficiency;Gaussian processes;Gaussian noise;Time measurement;Delay effects},
	doi={10.1109/TAES.2005.1468761},
	month={April},
}

@misc{spqrl,
	title={SPQReL Team description paper},
	author={M.T. L\'{a}zaro and L. Iocchi and D. Nardi and J. P. Fentanes and M. Hanheide},
	year={2018},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2018-sspl-spqrel.pdf}}
}

@inproceedings{Bastianelli:2016:DAG:3060832.3061005,
	author = {Bastianelli, Emanuele and Croce, Danilo and Vanzo, Andrea and Basili, Roberto and Nardi, Daniele},
	title = {A Discriminative Approach to Grounded Spoken Language Understanding in Interactive Robotics},
	booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
	series = {IJCAI'16},
	year = {2016},
	isbn = {978-1-57735-770-4},
	location = {New York, New York, USA},
	pages = {2747--2753},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=3060832.3061005},
	acmid = {3061005},
	publisher = {AAAI Press},
}

@misc{robofei,
	title={RoboFei Team description paper},
	author={Bruno F. V. Perez and
	Douglas R. Meneghetti and
	Enrico Matiuci and
	Leonardo C. Neves and
	Fagner Pimentel and
	Gabriel S. Melo and
	João Victor M. Santos and
	Lucas I. Gazignato and
	Marina Y. Gonbata and
	Mateus G. Carvalho and
	Matheus V. Domingos and
	Rodrigo C. Techi and
	Thiago S. B. Meyer and
	William Y. Yaguiu and
	Flavio Tonidandel and
	Reinaldo Bianchi and
	Plinio T. Aquino Junior},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-robofeiathome.pdf}}
}

@misc{techu,
	title={Tech United Team description paper},
	author={M.F.B. van der Burgh and
	J.J.M. Lunenburg and
	R.P.W. Appeldoorn and
	R.W.J. Wijnands and
	T.T.G. Clephas and
	M.J.J. Baeten and
	L.L.A.M. van Beek and
	R.A. Ottervanger and
	S. Aleksandrov and
	T. Assman, K. Dang and
	J. Geijsberts and
	L.G.L. Janssen and
	H.W.A.M. van Rooy and
	A.T. Hofkamp and
	M.J.G. van de Molengraft},
	year={2018},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2018-opl-techunited_eindhoven.pdf}}
}

@misc{walkingm,
	title={Walking Machine Team description paper},
	author={Jeffrey Cousineau and Huynh-Anh Le, et al.},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-walkingmachine.pdf}}
}

@article{GRONDIN201963,
	title = {Lightweight and optimized sound source localization and tracking methods for open and closed microphone array configurations},
	journal = {Robotics and Autonomous Systems},
	volume = {113},
	pages = {63 - 80},
	year = {2019},
	issn = {0921-8890},
	doi = {https://doi.org/10.1016/j.robot.2019.01.002},
	url = {http://www.sciencedirect.com/science/article/pii/S0921889017309399},
	author = {François Grondin and François Michaud},
	keywords = {Sound source localization, Sound source tracking, Microphone array, Online processing, Embedded system, Mobile robot, Robot audition},
	abstract = {Human–robot interaction in natural settings requires filtering out the different sources of sounds from the environment. Such ability usually involves the use of microphone arrays to localize, track and separate sound sources online. Multi-microphone signal processing techniques can improve robustness to noise but the processing cost increases with the number of microphones used, limiting response time and widespread use on different types of mobile robots. Since sound source localization methods are the most expensive in terms of computing resources as they involve scanning a large 3D space, minimizing the amount of computations required would facilitate their implementation and use on robots. The robot’s shape also brings constraints on the microphone array geometry and configurations. In addition, sound source localization methods usually return noisy features that need to be smoothed and filtered by tracking the sound sources. This paper presents a novel sound source localization method, called SRP-PHAT-HSDA, that scans space with coarse and fine resolution grids to reduce the number of memory lookups. A microphone directivity model is used to reduce the number of directions to scan and ignore non significant pairs of microphones. A configuration method is also introduced to automatically set parameters that are normally empirically tuned according to the shape of the microphone array. For sound source tracking, this paper presents a modified 3D Kalman (M3K) method capable of simultaneously tracking in 3D the directions of sound sources. Using a 16-microphone array and low cost hardware, results show that SRP-PHAT-HSDA and M3K perform at least as well as other sound source localization and tracking methods while using up to 4 and 30 times less computing resources respectively.}
}

@misc{kamerider,
	title={Kamerider Team description paper},
	author={Jeffrey Too Chuan Tan},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-kamerider_opl.pdf}}
}

@misc{hibikino,
	title={Hibikino Musashi Team description paper},
	author={Yuichiro Tanaka and
	 Yutaro Ishida and
	 Yushi Abe and
	 Tomohiro Ono and
	 Kohei Kabashima and 
	 Takuma Sakata and
	 Masashi Fukuyado and
	 Fuyuki Muto and
	 Takumi Yoshii and
	 Kazuki	Kanamaru and
	 Daichi Kamimura and
	 Kentaro Nakamura and
	 Yuta Nishimura and
	 Takashi Morie and
	 Hakaru Tamukoh},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-dspl-hibikino-musashiathome.pdf}}
}

@misc{tobitdp,
	title={Team of Bielefeld Team description paper},
	author={Sven Wachsmuth and
	Florian Lier and
	Leroy R\¨{u}gemer and
	Sebastian Meyer zu Borgsen},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-kamerider_opl.pdf}}
}

@inproceedings{music,
	author = {Otsuka, Takuma and Nakadai, Kazuhiro and Ogata, Tetsuya and Okuno, Hiroshi},
	year = {2011},
	month = {01},
	pages = {3109-3112},
	title = {Bayesian Extension of MUSIC for Sound Source Localization and Tracking.},
	journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH}
}

@INPROCEEDINGS{7952744,
	author={H. {Pan} and R. {Scheibler} and E. {Bezzam} and I. {Dokmanić} and M. {Vetterli}},
	booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	title={FRIDA: FRI-based DOA estimation for arbitrary array layouts},
	year={2017},
	pages={3186-3190},
	keywords={covariance matrices;direction-of-arrival estimation;FRIDA;FRI-based DOA estimation;arbitrary array layouts;directions of arrival estimation;spatial covariance matrix;Microphones;Direction-of-arrival estimation;Wideband;Signal resolution;Signal to noise ratio;Multiple signal classification;Geometry;Direction of arrival;finite rate of innovation;subspace method;search-free;wideband sources},
	doi={10.1109/ICASSP.2017.7952744},
	month={March},
}

@INPROCEEDINGS{1239145,
	author={A. B. {Gershman}},
	booktitle={4th International Conference on Antenna Theory and Techniques (Cat. No.03EX699)},
	title={Robust adaptive beamforming: an overview of recent trends and advances in the field},
	year={2003},
	volume={1},
	pages={30-35 vol.1},
	keywords={array signal processing;adaptive signal processing;optimisation;adaptive arrays;adaptive beamforming algorithms;array imperfections;environmental imperfections;robust adaptive beamforming;worst-case performance optimization;array response mismatch;diagonal loading;uncertainty set;ad hoc modifications;Robustness;Array signal processing;Adaptive arrays;Microphone arrays;Antenna arrays;Sonar;Radar antennas;Radar imaging;Wireless communication;Speech processing},
	doi={10.1109/ICATT.2003.1239145},
	month={Sep.},
}

@conference {288,
	title = {ROS: an open-source Robot Operating System},
	booktitle = {ICRA Workshop on Open Source Software},
	year = {2009},
	attachments = {http://www.willowgarage.com/sites/default/files/icraoss09-ROS.pdf},
	author = {Morgan Quigley and Ken Conley and Brian P. Gerkey and Josh Faust and Tully Foote and Jeremy Leibs and Rob Wheeler and Andrew Y. Ng}
}

@inproceedings{opdenAkker:2009:YAR:1708376.1708379,
	author = {op den Akker, Harm and op den Akker, Rieks},
	title = {Are You Being Addressed?: Real-time Addressee Detection to Support Remote Participants in Hybrid Meetings},
	booktitle = {Proceedings of the SIGDIAL 2009 Conference: The 10th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
	series = {SIGDIAL '09},
	year = {2009},
	isbn = {978-1-932432-64-0},
	location = {London, United Kingdom},
	pages = {21--28},
	numpages = {8},
	url = {http://dl.acm.org/citation.cfm?id=1708376.1708379},
	acmid = {1708379},
	publisher = {Association for Computational Linguistics},
	address = {Stroudsburg, PA, USA},
}

@article{DBLP:journals/corr/abs-1003-4083,
	author    = {Lindasalwa Muda and
	Mumtaj Begam and
	I. Elamvazuthi},
	title     = {Voice Recognition Algorithms using Mel Frequency Cepstral Coefficient
	{(MFCC)} and Dynamic Time Warping {(DTW)} Techniques},
	journal   = {CoRR},
	volume    = {abs/1003.4083},
	year      = {2010},
	url       = {http://arxiv.org/abs/1003.4083},
	archivePrefix = {arXiv},
	eprint    = {1003.4083},
	timestamp = {Mon, 13 Aug 2018 16:46:48 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1003-4083},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{1048137,
	author={T. {Choudhury} and J. M. {Rehg} and V. {Pavlovic} and A. {Pentland}},
	booktitle={Object recognition supported by user interaction for service robots},
	title={Boosting and structure learning in dynamic Bayesian networks for audio-visual speaker detection},
	year={2002},
	volume={3},
	pages={789-794 vol.3},
	keywords={belief networks;speaker recognition;image recognition;inference mechanisms;structure learning;dynamic Bayesian networks;audio-visual speaker detection;modeling tool;inference;boosted parameter learning;AdaBoost;Boosting;Intelligent networks;Bayesian methods;Speech;Testing;Lips;Laboratories;Educational institutions;Computer networks;Petroleum},
	doi={10.1109/ICPR.2002.1048137},
	month={Aug},
}

@inproceedings{lookwhostalking,
	author = {Cutler, Ross and Davis, L.},
	year = {2000},
	month = {02},
	pages = {1589 - 1592 vol.3},
	title = {Look who's talking: speaker detection using video and audio correlation},
	isbn = {0-7803-6536-4},
	doi = {10.1109/ICME.2000.871073}
}

@INPROCEEDINGS{840663,
	author={A. {Garg} and V. {Pavlovic} and J. M. {Rehg}},
	booktitle={Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)},
	title={Audio-visual speaker detection using dynamic Bayesian networks},
	year={2000},
	pages={384-390},
	keywords={belief networks;speech-based user interfaces;gesture recognition;image sequences;sensor fusion;inference mechanisms;audio-visual speaker detection;dynamic Bayesian networks;human-computer interfaces;ambiguous sensory data sequences;temporal fusion;multiple sensors;statistical inference;learning;contextual knowledge;audio/visual speaker detection;optimal fusion;Genie Casino Kiosk;Bayesian methods;Humans;Application software;Read only memory;Sensor fusion;Mouth;Computer interfaces;Microwave integrated circuits;Computer networks;Expert systems},
	doi={10.1109/AFGR.2000.840663},
	month={March},
}

@article{whosaidthat,
	author = {Chung, Joon Son and Lee, Bong-Jin and Han, Icksang},
	year = {2019},
	month = {06},
	title = {Who said that?: Audio-visual speaker diarisation of real-world meetings}
}

@INPROCEEDINGS{6926324,
	author={O. {Mubin} and J. {Henderson} and C. {Bartneck}},
	booktitle={The 23rd IEEE International Symposium on Robot and Human Interactive Communication},
	title={You just do not understand me! Speech Recognition in Human Robot Interaction},
	year={2014},
	pages={637-642},
	keywords={human-robot interaction;microphones;natural language processing;speech recognition;speech recognition friendly artificial language;English;ROILA;robot microphones;robot head movement;human robot interaction;Speech recognition;Microphones;Accuracy;Speech;Robot sensing systems;Headphones},
	doi={10.1109/ROMAN.2014.6926324},
	month={Aug},
}

@article{mediaequation,
	author = {Reeves, Byron and Nass, Clifford},
	year = {1996},
	month = {01},
	title = {The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places},
	journal = {Bibliovault OAI Repository, the University of Chicago Press}
}

@Article{Ivaldi2017,
	author={Ivaldi, Serena
	and Lefort, Sebastien
	and Peters, Jan
	and Chetouani, Mohamed
	and Provasi, Joelle
	and Zibetti, Elisabetta},
	title={Towards Engagement Models that Consider Individual Factors in HRI: On the Relation of Extroversion and Negative Attitude Towards Robots to Gaze and Speech During a Human--Robot Assembly Task},
	journal={International Journal of Social Robotics},
	year={2017},
	month={Jan},
	day={01},
	volume={9},
	number={1},
	pages={63--86},
	abstract={Estimating the engagement is critical for human--robot interaction. Engagement measures typically rely on the dynamics of the social signals exchanged by the partners, especially speech and gaze. However, the dynamics of these signals are likely to be influenced by individual and social factors, such as personality traits, as it is well documented that they critically influence how two humans interact with each other. Here, we assess the influence of two factors, namely extroversion and negative attitude toward robots, on speech and gaze during a cooperative task, where a human must physically manipulate a robot to assemble an object. We evaluate if the score of extroversion and negative attitude towards robots co-variate with the duration and frequency of gaze and speech cues. The experiments were carried out with the humanoid robot iCub and N = 56 adult participants. We found that the more people are extrovert, the more and longer they tend to talk with the robot; and the more people have a negative attitude towards robots, the less they will look at the robot face and the more they will look at the robot hands where the assembly and the contacts occur. Our results confirm and provide evidence that the engagement models classically used in human--robot interaction should take into account attitudes and personality traits.},
	issn={1875-4805},
	doi={10.1007/s12369-016-0357-8},
	url={https://doi.org/10.1007/s12369-016-0357-8}
}

@inproceedings{cao2018openpose,
	author = {Zhe Cao and Gines Hidalgo and Tomas Simon and Shih-En Wei and Yaser Sheikh},
	booktitle = {arXiv preprint arXiv:1812.08008},
	title = {Open{P}ose: realtime multi-person 2{D} pose estimation using {P}art {A}ffinity {F}ields},
	year = {2018}
}

@article{yolov3,
	title={YOLOv3: An Incremental Improvement},
	author={Redmon, Joseph and Farhadi, Ali},
	journal = {arXiv},
	year={2018}
}