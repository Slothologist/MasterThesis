@misc{JACK,
	title={Jack Audio Connection Kit},
	author={Paul Davis AND Stéphane Letz AND others},
	year={2002},
	howpublished={Open Source Project},
	note={\url{http://jackaudio.org/}}
}

@misc{Gstreamer,
	title={Gstreamer},
	author={Erik Walthinsen AND others},
	month={1},
	year={2001},
	howpublished={Open Source Project},
	note={\url{https://gstreamer.freedesktop.org/}}
}

@misc{rulebook_2018,
	author = {Matamoros, Mauricio AND Rascon, Caleb AND Hart, Justin AND Holz, Dirk AND van Beek, Loy },
	title = {RoboCup@Home 2018: Rules and Regulations},
	pages = {64 -- 67},
	year = {2018},
	howpublished = {\url{http://www.robocupathome.org/rules/2018_rulebook.pdf}},
}

@misc{psutil,
	title={psutil},
	author={Giampaolo Rodola AND others},
	year={2013},
	howpublished={Open Source Project},
	note={\url{https://psutil.readthedocs.io}}
}

@misc{ToBi,
	title={Team of Bielefeld},
	year={2009},
	howpublished={RoboCup@Home Participants},
	note={\url{https://www.cit-ec.de/en/tobi}}
}

@misc{GoogleSpeech,
	title={Google Cloud Speech-to-Text},
	author={Google},
	year={2013},
	month={11},
	day={6},
	howpublished={Software Product},
	note={\url{https://cloud.google.com/speech-to-text/}}
}

@misc{MicrosoftSpeech,
	title={Microsoft Speech to Text},
	author={Microsoft},
	year={2018},
	month={9},
	howpublished={Software Product},
	note={\url{https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/}}
}

@article{pyroomacoustics,
	author = {Robin Scheibler AND Eric Bezzam AND Ivan Dokmanić},
	title = {Pyroomacoustics: A Python package for audio room simulations and array processing algorithms},
	year = {2017},
	eprint = {arXiv:1710.04196},
	doi = {10.1109/ICASSP.2018.8461310},
}

@phdthesis{bonsai,
	type = {dissertation},
	title = {Behavior coordination for reusable system design in interactive robotics},
	author = {Siepmann Frederic},
	school = {Bielefeld University},
	year = {2013},
}

@article{deepspeech,
	author    = {Awni Y. Hannun and
	Carl Case and
	Jared Casper and
	Bryan Catanzaro and
	Greg Diamos and
	Erich Elsen and
	Ryan Prenger and
	Sanjeev Satheesh and
	Shubho Sengupta and
	Adam Coates and
	Andrew Y. Ng},
	title     = {Deep Speech: Scaling up end-to-end speech recognition},
	journal   = {CoRR},
	volume    = {abs/1412.5567},
	year      = {2014},
	url       = {http://arxiv.org/abs/1412.5567},
	archivePrefix = {arXiv},
	eprint    = {1412.5567},
	timestamp = {Mon, 13 Aug 2018 16:48:07 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/HannunCCCDEPSSCN14},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{tensorflow,
	title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url={http://tensorflow.org/},
	note={Software available from tensorflow.org},
	author={
	Mart\'{i}n~Abadi and
	Ashish~Agarwal and
	Paul~Barham and
	Eugene~Brevdo and
	Zhifeng~Chen and
	Craig~Citro and
	Greg~S.~Corrado and
	Andy~Davis and
	Jeffrey~Dean and
	Matthieu~Devin and
	Sanjay~Ghemawat and
	Ian~Goodfellow and
	Andrew~Harp and
	Geoffrey~Irving and
	Michael~Isard and
	Yangqing Jia and
	Rafal~Jozefowicz and
	Lukasz~Kaiser and
	Manjunath~Kudlur and
	Josh~Levenberg and
	Dan~Man\'{e} and
	Rajat~Monga and
	Sherry~Moore and
	Derek~Murray and
	Chris~Olah and
	Mike~Schuster and
	Jonathon~Shlens and
	Benoit~Steiner and
	Ilya~Sutskever and
	Kunal~Talwar and
	Paul~Tucker and
	Vincent~Vanhoucke and
	Vijay~Vasudevan and
	Fernanda~Vi\'{e}gas and
	Oriol~Vinyals and
	Pete~Warden and
	Martin~Wattenberg and
	Martin~Wicke and
	Yuan~Yu and
	Xiaoqiang~Zheng},
	year={2015},
}

@article{Nakadai_2017jrm,
	title={Development, Deployment and Applications of Robot Audition Open Source Software HARK},
	author={Kazuhiro Nakadai and Hiroshi G. Okuno and Takeshi Mizumoto},
	journal={Journal of Robotics and Mechatronics},
	volume={29},
	number={1},
	pages={16-25},
	year={2017},
	doi={10.20965/jrm.2017.p0016},
}

@INPROCEEDINGS{287895,
	author={A. T. {Alouani} and T. R. {Rice}},
	booktitle={Proceedings of 26th Southeastern Symposium on System Theory},
	title={On asynchronous data fusion},
	year={1994},
	pages={143-146},
	keywords={sensor fusion;delays;tracking;asynchronous data fusion;multisensor tracking system;sequential processing;inherent delay;multitasking radar;dissimilar sensors;Biosensors;Sensor systems;Sensor fusion;Acoustic sensors;Optical sensors;Laser radar;Time measurement;Delay;Radar measurements;Humans},
	doi={10.1109/SSST.1994.287895},
	month={March},
}

@article{TURK2014189,
	title = {Multimodal interaction: A review},
	journal = {Pattern Recognition Letters},
	volume = {36},
	pages = {189 - 195},
	year = {2014},
	issn = {0167-8655},
	doi = {https://doi.org/10.1016/j.patrec.2013.07.003},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865513002584},
	author = {Matthew Turk},
	keywords = {Multimodal interaction, Perceptual interface, Multimodal integration, Review},
	abstract = {People naturally interact with the world multimodally, through both parallel and sequential use of multiple perceptual modalities. Multimodal human–computer interaction has sought for decades to endow computers with similar capabilities, in order to provide more natural, powerful, and compelling interactive experiences. With the rapid advance in non-desktop computing generated by powerful mobile devices and affordable sensors in recent years, multimodal research that leverages speech, touch, vision, and gesture is on the rise. This paper provides a brief and personal review of some of the key aspects and issues in multimodal interaction, touching on the history, opportunities, and challenges of the area, especially in the area of multimodal integration. We review the question of early vs. late integration and find inspiration in recent evidence in biological sensory integration. Finally, we list challenges that lie ahead for research in multimodal human–computer interaction.},
}

@article{PORIA201650,
	title = {Fusing audio, visual and textual clues for sentiment analysis from multimodal content},
	journal = {Neurocomputing},
	volume = {174},
	pages = {50 - 59},
	year = {2016},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2015.01.095},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231215011297},
	author = {Soujanya Poria and Erik Cambria and Newton Howard and Guang-Bin Huang and Amir Hussain},
	keywords = {Multimodal fusion, Big social data analysis, Opinion mining, Multimodal sentiment analysis, Sentic computing},
	abstract = {A huge number of videos are posted every day on social media platforms such as Facebook and YouTube. This makes the Internet an unlimited source of information. In the coming decades, coping with such information and mining useful knowledge from it will be an increasingly difficult task. In this paper, we propose a novel methodology for multimodal sentiment analysis, which consists in harvesting sentiments from Web videos by demonstrating a model that uses audio, visual and textual modalities as sources of information. We used both feature- and decision-level fusion methods to merge affective information extracted from multiple modalities. A thorough comparison with existing works in this area is carried out throughout the paper, which demonstrates the novelty of our approach. Preliminary comparative experiments with the YouTube dataset show that the proposed multimodal system achieves an accuracy of nearly 80\%, outperforming all state-of-the-art systems by more than 20\%.},
}

@inproceedings{10.1117/12.138164,
	author = {W. Dale Blair and Theodore R. Rice and Brendan S. McDole and E. M. Sproul},
	title = {{Least-squares approach to asynchronous data fusion}},
	volume = {1697},
	booktitle = {Acquisition, Tracking, and Pointing VI},
	editor = {Michael K. Masten and Larry A. Stockum},
	organization = {International Society for Optics and Photonics},
	publisher = {SPIE},
	pages = {130 -- 141},
	year = {1992},
	doi = {10.1117/12.138164},
	URL = {https://doi.org/10.1117/12.138164}
}

@ARTICLE{4383603,
	author={L. P. {Yan} and B. S. {Liu} and D. H. {Zhou}},
	journal={IEEE Transactions on Aerospace and Electronic Systems},
	title={Asynchronous multirate multisensor information fusion algorithm},
	year={2007},
	volume={43},
	number={3},
	pages={1135-1146},
	keywords={recursive estimation;sensor fusion;state estimation;asynchronous multirate multisensor;optimal dynamic information fusion;state space models;state estimation;global measurements;linear minimum variance;State estimation;Signal processing algorithms;Sampling methods;Signal resolution;Sensor fusion;Recursive estimation;Stochastic processes;Image reconstruction;State-space methods},
	doi={10.1109/TAES.2007.4383603},
	month={July},
}

@ARTICLE{1468761,
	author={A. T. {Alouani} and J. E. {Gray} and D. H. {McCabe}},
	journal={IEEE Transactions on Aerospace and Electronic Systems},
	title={Theory of distributed estimation using multiple asynchronous sensors},
	year={2005},
	volume={41},
	number={2},
	pages={717-722},
	keywords={sensor fusion;target tracking;synchronisation;distributed estimation;multiple asynchronous sensors;track fusion;linear fusion rule;batch processing;Bar-Shalom-Campo fusion rule;Estimation theory;Sensor fusion;State estimation;Computer architecture;Delay estimation;Computational efficiency;Gaussian processes;Gaussian noise;Time measurement;Delay effects},
	doi={10.1109/TAES.2005.1468761},
	month={April},
}

@misc{spqrl,
	title={SPQReL Team description paper},
	author={M.T. L\'{a}zaro and L. Iocchi and D. Nardi and J. P. Fentanes and M. Hanheide},
	year={2018},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2018-sspl-spqrel.pdf}}
}

@inproceedings{Bastianelli:2016:DAG:3060832.3061005,
	author = {Bastianelli, Emanuele and Croce, Danilo and Vanzo, Andrea and Basili, Roberto and Nardi, Daniele},
	title = {A Discriminative Approach to Grounded Spoken Language Understanding in Interactive Robotics},
	booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
	series = {IJCAI'16},
	year = {2016},
	isbn = {978-1-57735-770-4},
	location = {New York, New York, USA},
	pages = {2747--2753},
	numpages = {7},
	url = {http://dl.acm.org/citation.cfm?id=3060832.3061005},
	acmid = {3061005},
	publisher = {AAAI Press},
}

@misc{robofei,
	title={RoboFei Team description paper},
	author={Bruno F. V. Perez and
	Douglas R. Meneghetti and
	Enrico Matiuci and
	Leonardo C. Neves and
	Fagner Pimentel and
	Gabriel S. Melo and
	João Victor M. Santos and
	Lucas I. Gazignato and
	Marina Y. Gonbata and
	Mateus G. Carvalho and
	Matheus V. Domingos and
	Rodrigo C. Techi and
	Thiago S. B. Meyer and
	William Y. Yaguiu and
	Flavio Tonidandel and
	Reinaldo Bianchi and
	Plinio T. Aquino Junior},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-robofeiathome.pdf}}
}

@misc{techu,
	title={Tech United Team description paper},
	author={M.F.B. van der Burgh and
	J.J.M. Lunenburg and
	R.P.W. Appeldoorn and
	R.W.J. Wijnands and
	T.T.G. Clephas and
	M.J.J. Baeten and
	L.L.A.M. van Beek and
	R.A. Ottervanger and
	S. Aleksandrov and
	T. Assman, K. Dang and
	J. Geijsberts and
	L.G.L. Janssen and
	H.W.A.M. van Rooy and
	A.T. Hofkamp and
	M.J.G. van de Molengraft},
	year={2018},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2018-opl-techunited_eindhoven.pdf}}
}

@misc{walkingm,
	title={Walking Machine Team description paper},
	author={Jeffrey Cousineau and Huynh-Anh Le, et al.},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-walkingmachine.pdf}}
}

@article{GRONDIN201963,
	title = {Lightweight and optimized sound source localization and tracking methods for open and closed microphone array configurations},
	journal = {Robotics and Autonomous Systems},
	volume = {113},
	pages = {63 - 80},
	year = {2019},
	issn = {0921-8890},
	doi = {https://doi.org/10.1016/j.robot.2019.01.002},
	url = {http://www.sciencedirect.com/science/article/pii/S0921889017309399},
	author = {François Grondin and François Michaud},
	keywords = {Sound source localization, Sound source tracking, Microphone array, Online processing, Embedded system, Mobile robot, Robot audition},
	abstract = {Human–robot interaction in natural settings requires filtering out the different sources of sounds from the environment. Such ability usually involves the use of microphone arrays to localize, track and separate sound sources online. Multi-microphone signal processing techniques can improve robustness to noise but the processing cost increases with the number of microphones used, limiting response time and widespread use on different types of mobile robots. Since sound source localization methods are the most expensive in terms of computing resources as they involve scanning a large 3D space, minimizing the amount of computations required would facilitate their implementation and use on robots. The robot’s shape also brings constraints on the microphone array geometry and configurations. In addition, sound source localization methods usually return noisy features that need to be smoothed and filtered by tracking the sound sources. This paper presents a novel sound source localization method, called SRP-PHAT-HSDA, that scans space with coarse and fine resolution grids to reduce the number of memory lookups. A microphone directivity model is used to reduce the number of directions to scan and ignore non significant pairs of microphones. A configuration method is also introduced to automatically set parameters that are normally empirically tuned according to the shape of the microphone array. For sound source tracking, this paper presents a modified 3D Kalman (M3K) method capable of simultaneously tracking in 3D the directions of sound sources. Using a 16-microphone array and low cost hardware, results show that SRP-PHAT-HSDA and M3K perform at least as well as other sound source localization and tracking methods while using up to 4 and 30 times less computing resources respectively.}
}

@misc{kamerider,
	title={Kamerider Team description paper},
	author={Jeffrey Too Chuan Tan},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-kamerider_opl.pdf}}
}

@misc{hibikino,
	title={Hibikino Musashi Team description paper},
	author={Yuichiro Tanaka and
	 Yutaro Ishida and
	 Yushi Abe and
	 Tomohiro Ono and
	 Kohei Kabashima and 
	 Takuma Sakata and
	 Masashi Fukuyado and
	 Fuyuki Muto and
	 Takumi Yoshii and
	 Kazuki	Kanamaru and
	 Daichi Kamimura and
	 Kentaro Nakamura and
	 Yuta Nishimura and
	 Takashi Morie and
	 Hakaru Tamukoh},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-dspl-hibikino-musashiathome.pdf}}
}

@misc{tobitdp,
	title={Team of Bielefeld Team description paper},
	author={Sven Wachsmuth and
	Florian Lier and
	Leroy R\¨{u}gemer and
	Sebastian Meyer zu Borgsen},
	year={2019},
	howpublished={Participation in RoboCup@Home, \url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/Team-Description-Papers}},
	note={\url{https://github.com/RoboCupAtHome/AtHomeCommunityWiki/wiki/files/tdp/2019-opl-kamerider_opl.pdf}}
}